{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "730e39a4",
   "metadata": {},
   "source": [
    "# Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67bcb11",
   "metadata": {},
   "source": [
    "Web scraping is the process of using bots to extract content and data from a website. Unlike screen scraping, which only copies pixels displayed onscreen, web scraping extracts underlying HTML code and, with it, data stored in a database. The scraper can then replicate entire website content elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf7a7e6",
   "metadata": {},
   "source": [
    "Web scraping typically extracts large amounts of data from websites for a variety of uses such as price monitoring, enriching machine learning models, financial data aggregation, monitoring consumer sentiment, news tracking, etc. Browsers show data from a website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61ecf8",
   "metadata": {},
   "source": [
    "# Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98a62a",
   "metadata": {},
   "source": [
    "Here are a few techniques commonly used to scrape data from websites. In general, all web scraping techniques retrieve content from websites, process it using a scraping engine, and generate one or more data files with the extracted content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed760d32",
   "metadata": {},
   "source": [
    "HTML Parsing:\n",
    "\n",
    "HTML parsing involves the use of JavaScript to target a linear or nested HTML page. It is a powerful and fast method for extracting text and links (e.g. a nested link or email address), scraping screens and pulling resources.\n",
    "\n",
    "DOM Parsing:\n",
    "\n",
    "The Document Object Model (DOM) defines the structure, style and content of an XML file. Scrapers typically use a DOM parser to view the structure of web pages in depth. DOM parsers can be used to access the nodes that contain information and scrape the web page with tools like XPath. For dynamically generated content, scrapers can embed web browsers like Firefox and Internet Explorer to extract whole web pages (or parts of them).\n",
    "\n",
    "Vertical Aggregation:\n",
    "\n",
    "Companies that use extensive computing power can create vertical aggregation platforms to target particular verticals. These are data harvesting platforms that can be run on the cloud and are used to automatically generate and monitor bots for certain verticals with minimal human intervention. Bots are generated according to the information required to each vertical, and their efficiency is determined by the quality of data they extract.\n",
    "\n",
    "XPath:\n",
    "\n",
    "XPath is short for XML Path Language, which is a query language for XML documents. XML documents have tree-like structures, so scrapers can use XPath to navigate through them by selecting nodes according to various parameters. A scraper may combine DOM parsing with XPath to extract whole web pages and publish them on a destination site.\n",
    "\n",
    "Google Sheets:\n",
    "\n",
    "Google Sheets is a popular tool for data scraping. Scarpers can use the IMPORTXML function in Sheets to scrape from a website, which is useful if they want to extract a specific pattern or data from the website. This command also makes it possible to check if a website can be scraped or is protected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289740c",
   "metadata": {},
   "source": [
    "# Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8bf6e",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beefcbe",
   "metadata": {},
   "source": [
    "# Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85ded8",
   "metadata": {},
   "source": [
    "Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489b4d6",
   "metadata": {},
   "source": [
    "# Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a301b",
   "metadata": {},
   "source": [
    "We have firstly used  code pipeline  to get code from github in which while creating new pipeline firstly we gave pipeline name and then selected the source i.e github(version 1) and done github authentication and then gave repositary name and branch and then selected the deploy provider AWS Elastic Beanstalk and then wrote application name and environment name , then we used AWS Elastic Beanstalk which will provide resources to run my application so firstly we created new application in AWS Elastic Beanstalk there we gave application name and then selected python platform and version and then it will take few minutes to create the application and once created so then firstly pipeline will be created and then deployment will be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e17c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
